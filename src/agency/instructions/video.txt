In my last video, I showed a new method proposed by Anthropic called MCP code execution. This technique is the future of building AI agents because it reduces token consumption 98% while giving your agents way more autonomy and flexibility to evolve their own skills. In this video, I'll build this life with you step by step. I'll compare this new approach with the old direct MCP method and give you the final repo plus a template to convert any MCP server to this new pattern. At the end, I'll also share whether this is actually ready for production or not. Before you watch this video, make sure you watch the previous one to fully understand all the theory behind this new method. Let's dive right in. So the agent that we will build today is the sales operations agent from Antropics blog post that can read meeting transcripts and then attach them into your CRM without actually reading the contents of the file. The architecture for this agent is pretty simple. It's just one agent with this code execution tool that connects to Google Drive MCP and Notion MCP because our CRM is in notion. So in order to get started, you need to copy our starter template which will now contain a new special command to use this new pattern. Make sure to set visibility to private and then open this repo in cursor. So I will be building this in cursor, but you can also use cloud codeex or any other AI coding agent that you prefer. The only thing you need to do if you're not using cursor is to simply tag this workflow file and the commands files that we'll be using in the prompt. So I'm going to start simple and just tell cloudset to create a sales ops agent with two built-in tools IPython interpreter and persistent shell tool. The reason we need these two tools is because the agent must be able to actually discover local files and then run them in Python in order to use this new approach. And both of these tools are already available in our framework. So let's hit send and wait until the agent is built. Now, I'm also going to tell it to add another sales ops direct MCP agent for comparison later. By the way, if you're building a more sophisticated MVP, there is also a new PRD command which you can use to have more control over your agent structure. Okay, now here's where the magic happens. I added a new special command called MCP code execution, which you can use to add MCP servers to an agent using this new pattern. So all you need to do in order to use this is simply tag this command and then tell cursor which MCP servers to use. So I recommend finding the exact MCP servers you need on GitHub or elsewhere and then adding links back into cursor. Now simply hit send again. So this command already describes to cursor pretty much everything it needs to know about this new pattern and even links the blog post and all the other resources that it might need. This way cursor can reliably add it using the same structure that they proposed in this blog post. So you can see how now cursor created the server directory with the code for our servers and then it proceeds to creating the code for individual tools. You might also need to authenticate the server if it's using or off. Now it proceeds to creating files for individual tools. So you can see how now this server is made here directly in code instead of passing this description and the arguments directly into the agents context window. Cursor saves this description in code. So then whenever our agent needs to use this tool, it will simply read this file and then it will see the whole description and the arguments in order to use this tool. So this way again we don't load all the tools. We only load one tool that cursor actually needs to use. Okay. So in total cursor created 15 tools for notion MCP server and four tools for Google drive. It has also already tested notion MCP server. However, it wasn't able to test Google Drive because of missing credentials. So make sure you provide all the credentials up front. Do not repeat my mistake. Typically you can find instructions on how to get the credentials inside the readme of your MCP server. Then you can simply tell cursor to retest the server. Then you will also be required to authenticate. Okay. Now the next step is to write instructions. So for writing instructions, I also recommend you use this write instructions command and then simply tag the agent that you want to create the instructions for. Then cursor is going to ask you a few questions to properly define the context for your agent. So make sure you answer all these questions carefully and the most important one is the process and workflow. So prompting is key with this technique and in order for this agent to use it effectively, it must clearly understand the process. So basically the first step is to always check available skills in the Mnt skills folder. I'll explain why Mnt folder later then it should use the skill if such a skill is found for a specific task. If no skills are found then it should only read one tool that it needs to use and after that it should combine all such tools in order to complete a task successfully and finally provide suggestions for new skills to be added. So you can play around with this workflow. This is just a general process from the blog post. But I think there are a lot of innovations just from prompting the agent. So let's hit send. And in the meantime, I also want to add MCPS using the direct MCP approach to the other sales ops direct MCP agent. So for this, we also have a command. It's simply add MCP. And then I'm also just going to link the two MCP servers that we added to the previous agent. Okay, now we are ready to test this agency. So let's open it in terminal and just run python agency.py. Pi. Now let's perform a simple test. Let me ask it what's on this notion page. So first the agent listed the mount skills directory. Then it read the fetch tool file to understand all the parameters. And it seems like it ran into some issues with the imports. However, then at the end I was still able to read this page by performing this fetch tool request and it provided me with the preview of this file. Okay. Now the next step is to deploy this agent and then we'll compare it with the direct MCP approach. So let's push our changes to GitHub and now let's go to agency. So hit create new agency and then find this new agent repo. Then provide your keys from the end file. And now let's wait until our agency is deployed. Okay. So now our agency is deployed. So let's actually test it. So let me put it in a custom GPT. Let's select both agents and let's deploy it. Now let me send this task from Andropics blog post to copy the transcript from Google Drive and paste it into a page in notion. Let's see if it's actually going to be able to do it. Seems like it did read way too many files. Like for example, it even read this server file which it definitely doesn't need. And now finally it tells me that it was in fact able to do this. So let's check the notion page. And yes indeed we now get this transcript from our Q&A call exactly like it was on Google Drive. And also at the end it proposed to create this new skill for itself. So let me tell it to save the skill. So you can see that the agent told me that it saved the skills in the mount skills directory. So the reason you need to use this mount directory is because we now have persistent storage on our platform. And without this feature, it would not be possible to allow your agents to evolve and build their own skills over time. So in this mount directory essentially your agents can now save files like this and reference them across different chats. So now if I start a new chat and then send the same prompt again, the agent should take a lot less time and just use the skill in order to do this task quickly. So you can see how now it found the skill and read the file and the agent was able to still perform the same task. Okay, now let's quickly test another agent that uses direct MCP approach. So now you can see how this agent is reading this whole transcript with the G drive read file tool which hopefully is not going to cost me too much and it also read the notion page and now it's trying to add this transcript to this notion page. But as you can see it has to manually type this whole thing which is definitely going to cost me too much now. So you can see how it's literally spending like probably tens of thousands of tokens for no reason simply because it had to do one simple copy paste operation. Finally, it's finished. So now let's take a look at the traces and analyze the costs. So tracing is enabled by default in our framework. And in order to see the traces, all you need to do is go to dashboard and then simply find the name of your agency. Okay, so let's see this sales ops direct MCP agent. And in total, this agent consumed 32,000 tokens. This is just insane. And most of these tokens aren't even input tokens. It's also a lot of output tokens, which are extremely expensive. And with this new approach, the amount of tokens is only 12,000, which is a lot less, although it's still quite a bit. And I think the reason this is still consumes so many tokens is because this agent, as you can see, just performs way too many unnecessary tool calls. But I do think that it's completely possible to optimize it with more prompting. And when the agent used one of the existing skills, it only consumed 4,000 tokens to perform the same task, which is now around 10 times less. So in conclusion, I believe this approach is extremely powerful, but as they say, with great power comes great responsibility. While I was definitely able to reduce the token consumption from the intermediate tool outputs, the agent still consumed more tokens than it should have when calling the tools. It executed code way too many times. It read way too many unnecessary files and it just took too long to perform a simple task on the first attempt. However, the agent was still able to save a lot more tokens compared to the old direct MCP approach. Additionally, it was even able to self-improve by creating a new skill which cut down the number of code executions in subsequent runs. So, my final verdict is yes, this approach is ready for production, but it requires proper prompting. Prompting here is key. While the LLMs aren't trained for this new method yet, you have to carefully describe how to use this new pattern, otherwise they're going to make a lot of mistakes. But if you do it right, and I'm sure many of you will, with this repo, the benefits far outweigh the costs. You get way more autonomy and flexibility with only a minor drop in reliability. So yes, this is the new paradigm. Just let your agents run the code. We no longer have to create these abstractions on top of other abstractions because agents can just generate the code to do whatever they need by themselves. However, don't use it for simple agents like customer support. save it for more sophisticated general agents like analytics, research, or operations. For simple agents, it still doesn't make sense. And when you're ready to deploy this, check out our platform because infrastructure overhead is actually the biggest downside of this approach. And we're the only platform on the market that supports everything you need to run this out of the box. More advanced EI agent templates are coming on this channel soon.